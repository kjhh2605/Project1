# PRD (제품 요구사항 문서)

## 1. 문서 메타
- 작성일: 2026-02-16
- 작성자: Project1 Team
- 버전: v0.1 (MVP)
- 상태: draft

## 2. 문제 정의
기존 코딩테스트 플랫폼은 정답 통과 여부 중심이라, 실무에서 중요한 디버깅 역량(원인 분석, 개선 근거 제시)을 충분히 평가하지 못한다.

## 3. 목표/비목표
### 목표
- Java/Python 디버깅 문제를 제출/평가할 수 있는 MVP 구축
- 실행 결과 + 정적 분석 + AI 루브릭 평가를 결합한 피드백 제공
- 비용 효율적인 구조로 시작하고 트래픽 증가 시 수평 확장 가능하도록 설계

### 비목표
- 실시간 다자 협업 편집의 완전한 안정화(후순위)
- 60개 언어 완전 지원(초기엔 Java/Python 중심)

## 4. 대상 사용자
- 코딩테스트 준비생(중급 이상)
- 기술 면접 대비 학습자
- 디버깅 문제를 출제하는 운영자/멘토

## 5. 핵심 사용자 플로우
1. 사용자가 문제를 열고 버그 코드를 확인한다.
2. 수정 코드와 개선 설명을 제출한다.
3. 시스템이 실행 검증(Judge0), 정적 분석, AI 평가를 순차 수행한다.
4. 사용자는 점수/근거/개선 피드백을 확인한다.

## 6. MVP 범위
### 포함
- 문제 조회, 코드 편집/제출, 제출 이력, 평가 결과 조회
- Java/Python 실행/정적 분석
- 3단계 AI 캐스케이드 평가(Tier 1~3)
- SSE 기반 제출 상태 스트리밍

### 제외
- 대규모 대회 운영 기능
- 고급 커뮤니티 기능(댓글/토론/팔로우)

## 7. 성공 지표(KPI)
- 제출 성공률(시스템 오류 제외): 99%+
- 평균 평가 완료 시간(P95): 15초 이내
- AI 평가 재시도율: 3% 미만
- MVP 사용자 주간 재방문율: 25%+

## 8. 비기능 요구사항
- 성능: API P95 300ms 이하(평가 비동기 제외)
- 가용성: 월 99.5% 이상
- 보안: 샌드박스 다중 격리(gVisor + seccomp + cgroups)
- 관측성: 로그/메트릭/트레이스 기본 수집

## 9. 리스크/의존성
- Judge0/LLM 외부 의존 장애
- AI 평가 품질 편차 및 비용 급증
- 샌드박스 취약점(CVE) 대응 지연

## 10. 출시 계획
- 알파(내부): 핵심 제출/평가 파이프라인 검증
- 베타(초기 사용자): 피드백 품질 및 비용 최적화
- GA: 안정화 + 운영 자동화 + 확장 정책 적용
